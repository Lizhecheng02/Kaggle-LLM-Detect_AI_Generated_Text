{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44868, 5)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"./fake_test_essays.csv\")\n",
    "sub = pd.read_csv(\"sample_submission.csv\")\n",
    "org_train = pd.read_csv(\"train_essays.csv\")\n",
    "train = pd.read_csv(\"train_v2_drcat_02.csv\", sep=\",\")\n",
    "train = train.dropna()\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    IS_TRAIN_ON_FULL = True\n",
    "    HALF_TRAIN_SAMPLE = 22500\n",
    "    RANDOM_STATE = 42\n",
    "    LOWER_CASE = False\n",
    "    VOCAB_SIZE = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Using full training data-----\n",
      "The shape of training dataset is: (44868, 5)\n",
      "                                                text  label  \\\n",
      "0  Phones & Driving\\n\\nThe Majority of people in ...      0   \n",
      "1  Extracurricular activities are very important ...      0   \n",
      "2  In our society today you can attend classes ri...      0   \n",
      "3  Having a positive attitude is crucial for lead...      1   \n",
      "4  As an eighth-grade student, it is important to...      1   \n",
      "\n",
      "                            prompt_name                source  RDizzl3_seven  \n",
      "0                    Phones and driving       persuade_corpus          False  \n",
      "1  Mandatory extracurricular activities       persuade_corpus          False  \n",
      "2                     Distance learning       persuade_corpus          False  \n",
      "3             Seeking multiple opinions  mistral7binstruct_v1          False  \n",
      "4                     Distance learning  mistral7binstruct_v1          False  \n"
     ]
    }
   ],
   "source": [
    "if CFG.IS_TRAIN_ON_FULL:\n",
    "    print(\"-----Using full training data-----\")\n",
    "    train = train.drop_duplicates(subset=[\"text\"])\n",
    "    train = train.sample(len(train))\n",
    "    print(\"The shape of training dataset is:\", train.shape)\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    print(train.head())\n",
    "else:\n",
    "    print(\"-----Using partial training data-----\")\n",
    "    train = train.drop_duplicates(subset=[\"text\"])\n",
    "    train_label_0 = train[train[\"label\"] == 0]\n",
    "    train_label_1 = train[train[\"label\"] == 1]\n",
    "    train_label_0 = train_label_0.sample(\n",
    "        CFG.HALF_TRAIN_SAMPLE, random_state=CFG.RANDOM_STATE\n",
    "    )\n",
    "    train_label_1 = train_label_1.sample(\n",
    "        CFG.HALF_TRAIN_SAMPLE, random_state=CFG.RANDOM_STATE\n",
    "    )\n",
    "    train = pd.concat([train_label_0, train_label_1])\n",
    "    train = train.sample(len(train))\n",
    "    print(\"The shape of training dataset is:\", train.shape)\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用bpe_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a2ef83a38c4e26bca8198addb5e891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b322b94d5684c1a9c787e51c57ffdc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac08462b619413b99effcc42514a8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "raw_tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFC()] + [normalizers.Lowercase()] if CFG.LOWER_CASE else []\n",
    ")\n",
    "raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=CFG.VOCAB_SIZE,\n",
    "    special_tokens=special_tokens\n",
    ")\n",
    "\n",
    "dataset = Dataset.from_pandas(test[[\"text\"]])\n",
    "\n",
    "\n",
    "def train_corpus():\n",
    "    for i in tqdm(range(0, len(dataset), 100)):\n",
    "        yield dataset[i:i + 100][\"text\"]\n",
    "\n",
    "\n",
    "raw_tokenizer.train_from_iterator(train_corpus(), trainer=trainer)\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=raw_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "tokenized_texts_test = []\n",
    "for text in tqdm(test[\"text\"].tolist()):\n",
    "    tokenized_texts_test.append(tokenizer.tokenize(text))\n",
    "\n",
    "tokenized_texts_train = []\n",
    "for text in tqdm(train[\"text\"].tolist()):\n",
    "    tokenized_texts_train.append(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练第一种vectorizer，其中min_df=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2648\n",
      "4.57763671875e-05 MB\n",
      "4.57763671875e-05 MB\n",
      "The shape of X_train is: (44868, 2648)\n",
      "The shape of y_train is: (44868,)\n",
      "The shape of X_test is: (100, 2648)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dummy(text):\n",
    "    return text\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(3, 5),\n",
    "    lowercase=False,\n",
    "    sublinear_tf=True,\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    token_pattern=None,\n",
    "    strip_accents=\"unicode\",\n",
    "    # min_df=2\n",
    ")\n",
    "\n",
    "vectorizer.fit(tokenized_texts_test)\n",
    "vocab = vectorizer.vocabulary_\n",
    "print(len(vocab))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(3, 5),\n",
    "    lowercase=False,\n",
    "    sublinear_tf=True,\n",
    "    vocabulary=vocab,\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    token_pattern=None,\n",
    "    strip_accents=\"unicode\"\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(tokenized_texts_train)\n",
    "# print(sys.getsizeof(X_train) / 1024 ** 2, \"MB\")\n",
    "# X_train = X_train.astype(\"float16\")\n",
    "# print(sys.getsizeof(X_train) / 1024 ** 2, \"MB\")\n",
    "y_train = train[\"label\"].values\n",
    "# y_train = y_train.astype(\"float16\")\n",
    "X_test = vectorizer.transform(tokenized_texts_test)\n",
    "# X_test = X_test.astype(\"float16\")\n",
    "print(\"The shape of X_train is:\", X_train.shape)\n",
    "print(\"The shape of y_train is:\", y_train.shape)\n",
    "print(\"The shape of X_test is:\", X_test.shape)\n",
    "\n",
    "del vectorizer\n",
    "gc.collect()\n",
    "\n",
    "# num_features = X_train.shape[1]\n",
    "\n",
    "# # 使用卡方检验选择特征\n",
    "# k = int(num_features / 4)\n",
    "# chi2_selector = SelectKBest(chi2, k=k)\n",
    "# X_train_chi2_selected = chi2_selector.fit_transform(X_train, y_train)\n",
    "# print(sys.getsizeof(X_train_chi2_selected) / 1024 ** 2, \"MB\")\n",
    "# X_test_chi2_selected = chi2_selector.transform(X_test)\n",
    "\n",
    "# X_train = X_train_chi2_selected\n",
    "# X_test = X_test_chi2_selected\n",
    "# print(\"The shape of X_train is:\", X_train.shape)\n",
    "# print(\"The shape of y_train is:\", y_train.shape)\n",
    "# print(\"The shape of X_test is:\", X_test.shape)\n",
    "\n",
    "# # 使用SVD进行降维\n",
    "# n_components = int(num_features / 4)\n",
    "# svd = TruncatedSVD(n_components=n_components)\n",
    "# X_train_svd = svd.fit_transform(X_train)\n",
    "# X_test_svd = svd.transform(X_test)\n",
    "\n",
    "# X_train = hstack([X_train_chi2_selected, X_train_svd])\n",
    "# X_train = X_train.toarray()\n",
    "# X_test = hstack([X_test_chi2_selected, X_test_svd])\n",
    "# X_test = X_test.toarray()\n",
    "# print(\"The shape of X_train is:\", X_train.shape)\n",
    "# print(\"The shape of y_train is:\", y_train.shape)\n",
    "# print(\"The shape of X_test is:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.53881283 0.53850576 0.6222858  0.58731609 0.52364945 0.62042078\n",
      " 0.60866646 0.56848397 0.58584903 0.56760189 0.56564023 0.57985954\n",
      " 0.60645844 0.59398168 0.58928837 0.54044327 0.60125729 0.63241251\n",
      " 0.59929876 0.62794392 0.65315452 0.6016683  0.58326302 0.68192785\n",
      " 0.58485859 0.60828681 0.60963118 0.6334919  0.6087341  0.62729575\n",
      " 0.60580776 0.58965286 0.62330451 0.57786114 0.53594859 0.57232989\n",
      " 0.59014448 0.60404967 0.56913137 0.61911397 0.65971964 0.64818353\n",
      " 0.64105429 0.60843813 0.5743164  0.63580869 0.64393306 0.5586796\n",
      " 0.60126017 0.54429855 0.61876131 0.56925495 0.68689756 0.65182317\n",
      " 0.60022441 0.6024068  0.64136381 0.61449416 0.66299927 0.62727604\n",
      " 0.60260257 0.61798492 0.52539252 0.50700281 0.58287077 0.64189232\n",
      " 0.60422235 0.609958   0.55532397 0.60941932 0.6084609  0.61442972\n",
      " 0.59906258 0.67046835 0.59651568 0.64290669 0.60593206 0.54775224\n",
      " 0.58890601 0.54033921 0.62380132 0.67238744 0.58510836 0.6261165\n",
      " 0.61283816 0.6069759  0.56982841 0.59915086 0.61239358 0.64226877\n",
      " 0.61230598 0.67008946 0.57823425 0.61250899 0.54567191 0.59428664\n",
      " 0.53220539 0.67525969 0.64453149 0.66634878]\n"
     ]
    }
   ],
   "source": [
    "if len(test.text.values) <= 2:\n",
    "    sub.to_csv(\"submission.csv\", index=False)\n",
    "else:\n",
    "    mnb = MultinomialNB(alpha=0.0225)\n",
    "\n",
    "    # gnb = GaussianNB()\n",
    "\n",
    "    sgd_model = SGDClassifier(max_iter=9000, tol=1e-4, loss=\"modified_huber\")\n",
    "\n",
    "    weights = [0.10, 0.31]\n",
    "\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            (\"mnb\", mnb),\n",
    "            # (\"gnb\", gnb),\n",
    "            (\"sgd\", sgd_model)\n",
    "        ],\n",
    "        weights=weights,\n",
    "        voting=\"soft\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    gc.collect()\n",
    "    final_preds1 = ensemble.predict_proba(X_test)[:, 1]\n",
    "    print(final_preds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练第二种tokenizer，不使用min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(text):\n",
    "    return text\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(3, 5),\n",
    "    lowercase=False,\n",
    "    sublinear_tf=True,\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    token_pattern=None,\n",
    "    strip_accents=\"unicode\"\n",
    ")\n",
    "\n",
    "vectorizer.fit(tokenized_texts_test)\n",
    "vocab = vectorizer.vocabulary_\n",
    "print(len(vocab))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(3, 5),\n",
    "    lowercase=False,\n",
    "    sublinear_tf=True,\n",
    "    vocabulary=vocab,\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    token_pattern=None,\n",
    "    strip_accents=\"unicode\"\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(tokenized_texts_train)\n",
    "y_train = train[\"label\"].values\n",
    "X_test = vectorizer.transform(tokenized_texts_test)\n",
    "print(\"The shape of X_train is:\", X_train.shape)\n",
    "print(\"The shape of y_train is:\", y_train.shape)\n",
    "print(\"The shape of X_test is:\", X_test.shape)\n",
    "\n",
    "del vectorizer\n",
    "gc.collect()\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# 使用卡方检验选择特征\n",
    "k = int(num_features / 4)\n",
    "chi2_selector = SelectKBest(chi2, k=k)\n",
    "X_train_chi2_selected = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_test_chi2_selected = chi2_selector.transform(X_test)\n",
    "\n",
    "X_train = X_train_chi2_selected\n",
    "X_test = X_test_chi2_selected\n",
    "print(\"The shape of X_train is:\", X_train.shape)\n",
    "print(\"The shape of y_train is:\", y_train.shape)\n",
    "print(\"The shape of X_test is:\", X_test.shape)\n",
    "\n",
    "# # 使用SVD进行降维\n",
    "# n_components = int(num_features / 4)\n",
    "# svd = TruncatedSVD(n_components=n_components)\n",
    "# X_train_svd = svd.fit_transform(X_train)\n",
    "# X_test_svd = svd.transform(X_test)\n",
    "\n",
    "# X_train = hstack([X_train_chi2_selected, X_train_svd])\n",
    "# X_train = X_train.toarray()\n",
    "# X_test = hstack([X_test_chi2_selected, X_test_svd])\n",
    "# X_test = X_test.toarray()\n",
    "# print(\"The shape of X_train is:\", X_train.shape)\n",
    "# print(\"The shape of y_train is:\", y_train.shape)\n",
    "# print(\"The shape of X_test is:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(test.text.values) <= 2:\n",
    "    sub.to_csv(\"submission.csv\", index=False)\n",
    "else:\n",
    "    lgb_params = {\n",
    "        \"n_iter\": 3000,\n",
    "        \"verbose\": -1,\n",
    "        \"objective\": \"cross_entropy\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"learning_rate\": 0.0056,\n",
    "        \"colsample_bytree\": 0.7,\n",
    "        \"colsample_bynode\": 0.8\n",
    "    }\n",
    "    lgb = LGBMClassifier(**lgb_params)\n",
    "\n",
    "    cat = CatBoostClassifier(\n",
    "        iterations=3000,\n",
    "        verbose=0,\n",
    "        learning_rate=0.0056,\n",
    "        subsample=0.4,\n",
    "        allow_const_label=True,\n",
    "        loss_function=\"CrossEntropy\"\n",
    "    )\n",
    "\n",
    "    xgb_params = {\n",
    "        \"n_estimators\": 2500,\n",
    "        \"verbosity\": 1,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"colsample_bytree\": 0.6,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    xgb = XGBClassifier(**xgb_params)\n",
    "\n",
    "    weights = [0.28, 0.67]\n",
    "\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            (\"lgb\", lgb),\n",
    "            (\"cat\", cat)\n",
    "        ],\n",
    "        weights=weights,\n",
    "        voting=\"soft\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    gc.collect()\n",
    "    final_preds2 = ensemble.predict_proba(X_test)[:, 1]\n",
    "    print(final_preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[\"generated\"] = final_preds1 * 0.30 + final_preds2 * 0.70\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
