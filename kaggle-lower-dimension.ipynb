{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44868, 5)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"./fake_test_essays.csv\")\n",
    "sub = pd.read_csv(\"sample_submission.csv\")\n",
    "org_train = pd.read_csv(\"train_essays.csv\")\n",
    "train = pd.read_csv(\"train_v2_drcat_02.csv\", sep=\",\")\n",
    "train = train.dropna()\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    IS_TRAIN_ON_FULL = True\n",
    "    HALF_TRAIN_SAMPLE = 22500\n",
    "    RANDOM_STATE = 42\n",
    "    LOWER_CASE = False\n",
    "    VOCAB_SIZE = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Using full training data-----\n",
      "The shape of training dataset is: (44868, 5)\n",
      "                                                text  label  \\\n",
      "0  From reading the article, Driverless Cars Are ...      0   \n",
      "1  Have u ever thought about going to space to ex...      0   \n",
      "2  Dear U.S Senate,\\n\\nI think that we should abo...      0   \n",
      "3  The success of a program in a school can have ...      1   \n",
      "4  Dear Senator,\\n\\nThe Presidential Election is ...      0   \n",
      "\n",
      "                        prompt_name           source  RDizzl3_seven  \n",
      "0                   Driverless cars  persuade_corpus           True  \n",
      "1                   Exploring Venus  persuade_corpus           True  \n",
      "2  Does the electoral college work?  persuade_corpus           True  \n",
      "3                 Distance learning    chat_gpt_moth          False  \n",
      "4  Does the electoral college work?  persuade_corpus           True  \n"
     ]
    }
   ],
   "source": [
    "if CFG.IS_TRAIN_ON_FULL:\n",
    "    print(\"-----Using full training data-----\")\n",
    "    train = train.drop_duplicates(subset=[\"text\"])\n",
    "    train = train.sample(len(train))\n",
    "    print(\"The shape of training dataset is:\", train.shape)\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    print(train.head())\n",
    "else:\n",
    "    print(\"-----Using partial training data-----\")\n",
    "    train = train.drop_duplicates(subset=[\"text\"])\n",
    "    train_label_0 = train[train[\"label\"] == 0]\n",
    "    train_label_1 = train[train[\"label\"] == 1]\n",
    "    train_label_0 = train_label_0.sample(\n",
    "        CFG.HALF_TRAIN_SAMPLE, random_state=CFG.RANDOM_STATE\n",
    "    )\n",
    "    train_label_1 = train_label_1.sample(\n",
    "        CFG.HALF_TRAIN_SAMPLE, random_state=CFG.RANDOM_STATE\n",
    "    )\n",
    "    train = pd.concat([train_label_0, train_label_1])\n",
    "    train = train.sample(len(train))\n",
    "    print(\"The shape of training dataset is:\", train.shape)\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用bpe_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8699bab8f6624155bf48bef5e2a29aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e42216c89164f4fa1a5b30db5ec727e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cab3af39d004ea684072c15158f7205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "raw_tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFC()] + [normalizers.Lowercase()] if CFG.LOWER_CASE else []\n",
    ")\n",
    "raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=CFG.VOCAB_SIZE,\n",
    "    special_tokens=special_tokens\n",
    ")\n",
    "\n",
    "dataset = Dataset.from_pandas(test[[\"text\"]])\n",
    "\n",
    "\n",
    "def train_corpus():\n",
    "    for i in tqdm(range(0, len(dataset), 100)):\n",
    "        yield dataset[i:i + 100][\"text\"]\n",
    "\n",
    "\n",
    "raw_tokenizer.train_from_iterator(train_corpus(), trainer=trainer)\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=raw_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "tokenized_texts_test = []\n",
    "for text in tqdm(test[\"text\"].tolist()):\n",
    "    tokenized_texts_test.append(tokenizer.tokenize(text))\n",
    "\n",
    "tokenized_texts_train = []\n",
    "for text in tqdm(train[\"text\"].tolist()):\n",
    "    tokenized_texts_train.append(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练第一种vectorizer，其中min_df=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2648\n",
      "The shape of X_train is: (44868, 2648)\n",
      "The shape of y_train is: (44868,)\n",
      "The shape of X_test is: (100, 2648)\n",
      "The shape of X_train is: (44868, 1324)\n",
      "The shape of y_train is: (44868,)\n",
      "The shape of X_test is: (100, 1324)\n"
     ]
    }
   ],
   "source": [
    "def dummy(text):\n",
    "    return text\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(3, 5),\n",
    "    lowercase=False,\n",
    "    sublinear_tf=True,\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    token_pattern=None,\n",
    "    strip_accents=\"unicode\",\n",
    "    # min_df=2\n",
    ")\n",
    "\n",
    "vectorizer.fit(tokenized_texts_test)\n",
    "vocab = vectorizer.vocabulary_\n",
    "print(len(vocab))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(3, 5),\n",
    "    lowercase=False,\n",
    "    sublinear_tf=True,\n",
    "    vocabulary=vocab,\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    token_pattern=None,\n",
    "    strip_accents=\"unicode\"\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(tokenized_texts_train)\n",
    "y_train = train[\"label\"].values\n",
    "X_test = vectorizer.transform(tokenized_texts_test)\n",
    "print(\"The shape of X_train is:\", X_train.shape)\n",
    "print(\"The shape of y_train is:\", y_train.shape)\n",
    "print(\"The shape of X_test is:\", X_test.shape)\n",
    "\n",
    "del vectorizer\n",
    "gc.collect()\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# 使用卡方检验选择特征\n",
    "k = int(num_features / 4)\n",
    "chi2_selector = SelectKBest(chi2, k=k)\n",
    "X_train_chi2_selected = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_test_chi2_selected = chi2_selector.transform(X_test)\n",
    "\n",
    "# 使用SVD进行降维\n",
    "n_components = int(num_features / 4)\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "X_train_svd = svd.fit_transform(X_train)\n",
    "X_test_svd = svd.transform(X_test)\n",
    "\n",
    "X_train = hstack([X_train_chi2_selected, X_train_svd])\n",
    "X_train = X_train.toarray()\n",
    "X_test = hstack([X_test_chi2_selected, X_test_svd])\n",
    "X_test = X_test.toarray()\n",
    "print(\"The shape of X_train is:\", X_train.shape)\n",
    "print(\"The shape of y_train is:\", y_train.shape)\n",
    "print(\"The shape of X_test is:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31890795 0.32001686 0.6543361  0.61546712 0.30064467 0.65886842\n",
      " 0.64226561 0.59345745 0.61412808 0.34710491 0.59681671 0.61054089\n",
      " 0.64175147 0.62212826 0.37101698 0.31598659 0.62907721 0.66653172\n",
      " 0.62503337 0.65859744 0.6993701  0.63538895 0.61133781 0.72240251\n",
      " 0.60946216 0.64017266 0.64207154 0.67497707 0.63839117 0.65967951\n",
      " 0.64089736 0.36748701 0.66030573 0.36053034 0.31037571 0.34663465\n",
      " 0.36698914 0.63556327 0.59407003 0.65552769 0.70261429 0.68100483\n",
      " 0.68206439 0.64553832 0.35398476 0.67079062 0.68179057 0.33103208\n",
      " 0.63412254 0.31960534 0.65049827 0.35115054 0.72939797 0.68954357\n",
      " 0.37834812 0.6324813  0.67535065 0.64885375 0.70343303 0.66042592\n",
      " 0.63766377 0.64355716 0.30510121 0.28699442 0.36275033 0.67566586\n",
      " 0.63407982 0.64442699 0.33265975 0.64234061 0.6386944  0.6449804\n",
      " 0.62529667 0.73127398 0.37441792 0.67649822 0.64382642 0.32863988\n",
      " 0.36896994 0.32103906 0.65497233 0.72653654 0.6142969  0.66267559\n",
      " 0.64259946 0.63881725 0.3504765  0.38008424 0.64607193 0.68570673\n",
      " 0.64659112 0.71379988 0.35957677 0.64189871 0.33311912 0.38259433\n",
      " 0.31483463 0.71102462 0.67857362 0.71743551]\n"
     ]
    }
   ],
   "source": [
    "if len(test.text.values) <= 2:\n",
    "    sub.to_csv(\"submission.csv\", index=False)\n",
    "else:\n",
    "    mnb = MultinomialNB(alpha=0.0225)\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "\n",
    "    sgd_model = SGDClassifier(max_iter=9000, tol=1e-4, loss=\"modified_huber\")\n",
    "\n",
    "    weights = [0.10, 0.31]\n",
    "\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            # (\"mnb\", mnb),\n",
    "            (\"gnb\", gnb),\n",
    "            (\"sgd\", sgd_model)\n",
    "        ],\n",
    "        weights=weights,\n",
    "        voting=\"soft\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    gc.collect()\n",
    "    final_preds1 = ensemble.predict_proba(X_test)[:, 1]\n",
    "    print(final_preds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练第二种tokenizer，不使用min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2648\n",
      "The shape of X_train is: (44868, 2648)\n",
      "The shape of y_train is: (44868,)\n",
      "The shape of X_test is: (100, 2648)\n",
      "The shape of X_train is: (44868, 1324)\n",
      "The shape of y_train is: (44868,)\n",
      "The shape of X_test is: (100, 1324)\n"
     ]
    }
   ],
   "source": [
    "def dummy(text):\n",
    "    return text\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(3, 5),\n",
    "    lowercase=False,\n",
    "    sublinear_tf=True,\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    token_pattern=None,\n",
    "    strip_accents=\"unicode\"\n",
    ")\n",
    "\n",
    "vectorizer.fit(tokenized_texts_test)\n",
    "vocab = vectorizer.vocabulary_\n",
    "print(len(vocab))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(3, 5),\n",
    "    lowercase=False,\n",
    "    sublinear_tf=True,\n",
    "    vocabulary=vocab,\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    token_pattern=None,\n",
    "    strip_accents=\"unicode\"\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(tokenized_texts_train)\n",
    "y_train = train[\"label\"].values\n",
    "X_test = vectorizer.transform(tokenized_texts_test)\n",
    "print(\"The shape of X_train is:\", X_train.shape)\n",
    "print(\"The shape of y_train is:\", y_train.shape)\n",
    "print(\"The shape of X_test is:\", X_test.shape)\n",
    "\n",
    "del vectorizer\n",
    "gc.collect()\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# 使用卡方检验选择特征\n",
    "k = int(num_features / 4)\n",
    "chi2_selector = SelectKBest(chi2, k=k)\n",
    "X_train_chi2_selected = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_test_chi2_selected = chi2_selector.transform(X_test)\n",
    "\n",
    "# 使用SVD进行降维\n",
    "n_components = int(num_features / 4)\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "X_train_svd = svd.fit_transform(X_train)\n",
    "X_test_svd = svd.transform(X_test)\n",
    "\n",
    "X_train = hstack([X_train_chi2_selected, X_train_svd])\n",
    "X_train = X_train.toarray()\n",
    "X_test = hstack([X_test_chi2_selected, X_test_svd])\n",
    "X_test = X_test.toarray()\n",
    "print(\"The shape of X_train is:\", X_train.shape)\n",
    "print(\"The shape of y_train is:\", y_train.shape)\n",
    "print(\"The shape of X_test is:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 46\u001b[0m\n\u001b[0;32m     35\u001b[0m weights \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.28\u001b[39m, \u001b[38;5;241m0.67\u001b[39m]\n\u001b[0;32m     37\u001b[0m ensemble \u001b[38;5;241m=\u001b[39m VotingClassifier(\n\u001b[0;32m     38\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     39\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlgb\u001b[39m\u001b[38;5;124m\"\u001b[39m, lgb),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     45\u001b[0m )\n\u001b[1;32m---> 46\u001b[0m \u001b[43mensemble\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m     48\u001b[0m final_preds2 \u001b[38;5;241m=\u001b[39m ensemble\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\86183\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\86183\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py:349\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m    347\u001b[0m transformed_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[1;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86183\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py:81\u001b[0m, in \u001b[0;36m_BaseVoting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of `estimators` and weights must be equal; got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m     )\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVoting\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     92\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Uses 'drop' as placeholder for dropped estimators\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\86183\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86183\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86183\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\86183\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if len(test.text.values) <= 2:\n",
    "    sub.to_csv(\"submission.csv\", index=False)\n",
    "else:\n",
    "    lgb_params = {\n",
    "        \"n_iter\": 3000,\n",
    "        \"verbose\": -1,\n",
    "        \"objective\": \"cross_entropy\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"learning_rate\": 0.0056,\n",
    "        \"colsample_bytree\": 0.7,\n",
    "        \"colsample_bynode\": 0.8\n",
    "    }\n",
    "    lgb = LGBMClassifier(**lgb_params)\n",
    "\n",
    "    cat = CatBoostClassifier(\n",
    "        iterations=3000,\n",
    "        verbose=0,\n",
    "        learning_rate=0.0056,\n",
    "        subsample=0.4,\n",
    "        allow_const_label=True,\n",
    "        loss_function=\"CrossEntropy\"\n",
    "    )\n",
    "\n",
    "    xgb_params = {\n",
    "        \"n_estimators\": 2500,\n",
    "        \"verbosity\": 1,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"colsample_bytree\": 0.6,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    xgb = XGBClassifier(**xgb_params)\n",
    "\n",
    "    weights = [0.28, 0.67]\n",
    "\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            (\"lgb\", lgb),\n",
    "            (\"cat\", cat)\n",
    "        ],\n",
    "        weights=weights,\n",
    "        voting=\"soft\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    gc.collect()\n",
    "    final_preds2 = ensemble.predict_proba(X_test)[:, 1]\n",
    "    print(final_preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[\"generated\"] = final_preds1 * 0.30 + final_preds2 * 0.70\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
